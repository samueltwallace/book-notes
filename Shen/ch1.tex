\section{Introduction}

\subsection{Weighted Residual Methods}

Consider the following general problem:
\begin{equation}
	\partial_t u(x,t) -  \mathcal{L} u (x,t) =  \mathcal{N}(u)(x,t),  \hspace{8pt} t>0, x \in  \Omega
\end{equation}

Where $ \mathcal{L} $ is a leading spatial derivative operator, and $ \mathcal{N} $ is a lower-order linear or non-linear operator involving only spatial derivatives. Here, $ \Omega $ denotes a bounded domain of $ \mathbb{R}^d $, $ d=1,2, \text{ or } 3 $. This equation is to be supplemented with an initial condition and suitable boundary conditions. \\
\indent We shall only consider the WRM for the spatial discretization, and assume that the time derivative is discretized with a suitable time-stepping scheme. Among various time-stepping methods, semi-implicit schemes or linearly-implicit schemes, in which the principal linear operators are treated \textit{implicitly} to reduce the associated stability constraint, while the non-linear equations are treated explicitly to avoid the expensive process of solving nonlinear equations at each time step, are most frequently used int eh context of spectral methods. \\
\indent Let $ \tau $ be the step size, and $ u^k(\cdot) $ be an approximation of $ u(\cdot ,k \tau) $. As an example, we consider the Crank-Nicolson leap-frog scheme for the equation:
\begin{equation}
	\frac{u^{ n+1 } - u^{ n-1 }}{2 \tau} - \mathcal{L}\left( \frac{u^{ n+1 } + u^{ n-1 }}{2}  \right) =  \mathcal{N}\left( u^{ n } \right) \hspace{4pt} n \geq 1
\end{equation}

We can rewrite this as
\begin{equation}
	\mathbf{L}u(x) := \alpha u(x) - \mathcal{L}u(x) = f(x), \hspace{4pt} x \in \Omega
\end{equation}

where $ u = \frac{u^{ n+1 } + u^{ n+1 }}{2} , \alpha = \tau^{-1} $ and $ f = \alpha^{ n-1 } + \mathcal{N}\left( u^{ n } \right) $. Hence, at each time step, we need to solve a steady-state problem of the form of (3). \\
\indent At this point, it is important to emphasize that the construction of efficient numerical solvers for some important equations in the form of (3), such as Poisson-type equations and advection-diffusion equations, is an essential step in solving general nonlinear PDEs. With this in mind, a particular emphasis for equations of the form (3) where $ \mathcal{L} $ is a \textit{linear elliptic} operator. \\
\indent The starting point of the WRM is to approximate the solution $ u $ is to approximate (3) by a finite sum
\begin{equation}
	u(x) \approx u_N (x) = \sum_{ k=0 }^{ N } a_{ k } \phi_{ k } (x)
\end{equation}

where $ \{ \phi_{ k } \} $ are the \textit{trial (or basis) functions}, and the expansion coefficients are to be determined. Substituting $ u_{ N } $ for $ u $ in (3) leads to the \textit{residual}
\begin{equation}
	\mathbf{R}_{ N }(x) = \mathbf{L}u_{ N }(x) - f(x) \neq 0 \hspace{4pt} x \in \Omega
\end{equation}

The notion of the WRM is to force the residual to zero by requiring
\begin{equation}
	\left( \mathbf{R}_{ N }, \mathbf{\psi}_{ j } \right)_{ \omega } = \int_{ \Omega } \mathbf{R}_{ N }(x) \mathbf{\psi}_{ j }(x) \omega (x) dx = 0, \hspace{4pt} 0 \leq j \leq N
\end{equation}

where $ \{ \mathbf{\psi}_{ j } \} $ are the \textit{test functions}, and $ \omega $ is a positive weight function; or 
\begin{equation}
	\langle \mathbf{ R }_{ N }, \mathbf{ \psi }_{ j }  \rangle_{ N, \omega } := \sum_{ k=0 }^{ N } \mathbf{ R }_{ N } (x_{ k }) \mathbf{ \psi }_{ j } (x_{ k }) \omega_{ k } = 0, \hspace{4pt} 0 \leq j \leq N
\end{equation}

where $ \{ x_{ k } \}_{ k=0 }^{ N } $ are a set of preselected collocation points, and $ \{ \omega_{ k } \}_{ k=0 }^{ N } $ are the weights of a numerical quadrature formula. \\
\indent  The choice of trial/test functions is one of the main features that distinguishes spectral methods from finite-elements and finite-difference methods. In the latter two methods, the trial/test functions are local in character with finite regularities. In contrast, spectral methods employ globally smooth functions as trial/test functions. The most commonly used trial/test functions are trigonometric functions of orthogonal polynomials (typically, the eigenfunctions of singular Sturm-Liouville problems), which include

\begin{itemize}
	\item $ \phi_{ k } (x) = e^{ i k x } $ (Fourier spectral method)
	\item $ \phi_{ k }(x) = T_{ k }(x) $ (Chebyshev spectral method)
	\item $ \phi_{ k } = L_{ k }(x) $ (Legendre spectral method)
	\item $ \phi_{ k }= \mathcal{L}_{ k } (x) $ (Laguerre spectral method)
	\item $ \phi_{ k }(x) = H_{ k }(x) $ (Hermite spectral method) 
\end{itemize}

Here, $ T_{ k }, L_{ k }, \mathcal{L}_{ k }, \text{ and } H_{ k } $ are the Chebyshev, Legendre, Laguerre and Hermite polynomials of degree $ k $ respectively. \\
\indent The choice of test functions distinguishes the following formulations:

\begin{itemize}
	\item \textit{Galerkin.} The test functions are the same as the trial ones (i.e., $ \phi_{ k } = \psi_{ k } $ in (6) or (7)), assuming the boundary conditions are periodic of homogeneous.
	\item \textit{Petrov-Galerkin}. The test functions are different from the trial ones.
	\item \textit{Collocation}. The test functions $ \{ \psi_{ k } \} $ in (7) are the Lagrange basis polynomials such that $ \psi_{ k }(x_{ j }) = \delta_{ jk } $ , where $ \{ x_{ j } \} $  are preassigned collocation points. Hence, the residual is forced to zero at each $ x_{ j } $ , i.e. $ \mathbf{ R }_{ N } (x_{ j }) = 0 $. 
\end{itemize}

\begin{rmk}
In the  literature, the term of pseudo-spectral methods is often used to describe any spectral method where some operations involve a collocation approach or a numerical quadrature which produces aliasing errors. In this sense, almost all practical spectral methods are pseudo-spectral. In this book, we shall not classify a method as pseudo-spectral of spectral. Instead, it will be classified as Galerkin type or collocation type.
\end{rmk}

\begin{rmk}
	The so-called \textit{tau} method is a particular class of Petrov-Galerkin method. While the \textit{tau} method offers some advantages in certain situations, for most problems, it is usually better to use a well-designed Galerkin or Petrov-Galerkin method. So in this book, we shall not touch on this topic, and refer to the references therein for a thorough discussion of this approach.
\end{rmk}

In the forthcoming sections, we shall demonstrate how to construct spectral methods for solving differential equations by examining several spectral schemes based on Galerkin, Petrov-Galerkin, and collocation formulas in a general manner. We shall revisit these illustrative examples in a more rigorous fashion in the main body of the book.

\subsection{Spectral-Collocation Method}

To fix the idea, let us consider the following linear problem:

\begin{equation}
	\mathbf{ L }u(x) = -u''(x) + p(x) u'(x) + q(x) u(x) = f(x), \hspace{4pt} x \in (-1,1)
\end{equation}
\begin{equation}
	B_{ \pm }u(\pm 1) = g_{ \pm }
\end{equation}

Where $ B_{ \pm } $ are linear operators corresponding to Dirichlet, Neumann, or Robin boundary conditions, and the data $ p,q,f $ and $ g_{ \pm } $ are given such that the above problem is well-posed. \\
\indent As mentioned above, the collocation method forces the residual to vanish pointwisely at a set of preassigned points. More precisely, let $ \{ x_{ j } \}_{ j=0 }^{ N } $ (with $ x_{ 0 }= -1 $ and $ x_{ N } = 1 $ ) be a set of Gauss-Lobatto points (see Chap. 3), and let $ P_{ N } $ be the set of all real algebraic polynomials of degree $ \leq N $ . The spectral-collocation method for (8) amounts to finding $ u_{ N } \in P_{ N } $ such that

\begin{enumerate}
	\item the residual $ \mathbf{ R }_{ N }(x_{ k }) = \mathbf{ L }u_{ N }(x_{ k }) - f(x_{ k }) = 0, \hspace{4pt} 1 \leq K \leq N-1 $ 
	\item $ u_{ N } $ satisfies exactly the boundary conditions, i.e.,
		\begin{equation}
			B_{ - }u_{ N }(x_{ 0 }) = g_{ - }, \hspace{4pt} B_{ + }u_{ N }\left( x_{ N } \right) = g_{ + }
		\end{equation}
\end{enumerate}

The spectral-collocation method is usually implemented in the physical space by seeking approximate solution in the form
\begin{equation}
	u_{ N }(x) = \sum_{ j=0 }^{ N } u_{ N }(x_{ j }) h_{ j }(x)	
\end{equation}
where $ \{ h_{ j } \} $ are the Lagrange basis polynomials (also referred to as \textit{nodal} basis functions), i.e., $ h_{ j } \in P_{ N } $ and $ h_{ j } (x_{ k }) = \delta_{ jk } $. Hence, in inserting (11) into (9)-(10) leads to the linear system
\begin{equation}
	\sum_{ j=0 }^{ N } \left[ \mathbf{ L }h_{ j }(x_{ k }) \right] u_{ N }(x_{ j }) = f(x_{ k }), \hspace{4pt}
\end{equation}

\begin{equation}
	\sum_{ j=0 }^{ N } \left[ \mathbf{ L } h_{ j }(x_{ k }) \right] u_{ N }\left( x_{ j } \right) = f\left( x_{ k } \right), \hspace{4pt} 1 \leq k \leq N-1
\end{equation}

\begin{equation}
	\sum_{ j=0 }^{ N } \left[ B_{ - }j_{ j }\left( x_{ 0 } \right) \right] u_{ N } \left( x_{ j } \right) = g_{ - }, \sum_{ j=0 }^{ N } \left[ B_{ + } j_{ j } \left( x_{ N } \right) \right] u_{ N } (x_{ j }) = g_{ + }
\end{equation}

The above system contains $ N+1 $ unknowns, so we can rewrite it in a matrix form. To fix the idea, we consider (8) with Dirchlet boudnary conditions: $ u( \pm 1) = g_{ \pm } $. In this case, setting $ u_{ N }\left( x_{ 0 } \right) g_{ - } $ and $ u_{ N } \left( x_{ N } \right) = g_{ + } $  in the first equation of (12) reduces to 
\begin{equation}
	\sum_{ j=1 }^{ N-1 } \left[ \mathbf{ L } h_{ j } (x_{ k }) \right] u_{ N } \left( x_{ j } \right) = f(x_{ k }) - \left\{ \left[ \mathbf{ L }h_{ 0 }\left( x_{ k } \right) \right]g_{ - } + \left[ \mathbf{ L } h_{ N } (x_{ k }) \right] g_{ + } \right\}
\end{equation}

for $ 1 \leq K \leq N-1 $. Differentiating (11) $ m $ times leads to 
\begin{equation}
	u_{ N }^{ \left( m \right) } \left( x_{ k } \right) = \sum_{ j=0 }^{ N } d_{ kj }^{ \left( m \right) } u_{ N } \left( x_{ j } \right)
\end{equation}
where
\begin{equation}
	d_{ kj }^{ \left( m \right) } = h_{ j }^{ \left( m \right) } \left( x_{ k } \right)
\end{equation}


The matrix $ D^{ (m) } \left( d_{ kj }^{ (m) } \right)_{ k,j=0 \ldots N } $ is called the differentiation matrix of order $ m $ relative to the $ \{ s_{ j } \}_{ j=0 }^{ N } $. If we denote by $ \mathbf{ u }^{ (m) } $ the vector whose components are the values of $ u_{ N }^{ (m) } $ at the collocation points, it follows from (14) that 
\begin{equation}
	\mathbf{ u }^{ (m) } = D^{ (m) } \mathbf{ u }^{ (0) }, \hspace{4pt} m \geq 1
\end{equation}

Hence, we have
\begin{equation}
	\mathbf{ L } h_{ j } \left( x_{ k } \right) = - d^{ (2) }_{ kj } + p(x_{ k }) d^{ (1) }_{ kj } + q\left( x_{ k } \right) \delta_{ kj }
\end{equation}

Denote by $ \mathbf{ f } $ the vector with $ N-1 $ components given by the right-handside of (13). Setting
\begin{equation}
	\tilde{D}_{ m } = \left( d_{ kj }^{ (m) } \right)_{ kj=1,\ldots,N }, \hspace{4pt} m=1,2
\end{equation}

\[
	P = \mathrm{diag} \left( p(x_{ 1 }), \ldots, p(x_{ N-1 }) \right), \hspace{4pt} Q = \mathrm{ diag } \left( q(x_{ 1 }), \ldots, q(x_{ N-1 }) \right)
\]

the system (13) reduces to
\begin{equation}
	\left( - \tilde{D}_{ 2 } + P \tilde{D}_{ 1 } + Q \right) \mathbf{ u }^{ (0) } = \mathbf{ f }
\end{equation}

Observe that the collocation method is easy to implement, once the differentiation matrices are precomputed. Moreover, it is very convenient for solving problems with variable coefficients and/or nonlinear problems, since we work in the physical space and derivatives can be valuated by (14) directly. As a result, the collocation method has been extensively used in practice. However, three important issues should be considered in the implementation and analysis of a collocation method:

\begin{itemize}
	\item The coefficient matrix of the collocation system is always full with a condition number behaving like $ O\left( N^{ 2m } \right) $ ( $ m $ is the order of the differential equation).
	\item The choice of collocation points is crucial in terms of stability, accuracy, and ease of dealing with boudnary conditions. In general, they are chosen as nodes (typically, zeros of orthogonal polynomials) of Gauss-type quadrature formulas.
	\item The aforementioned collocation scheme is formulated in a \textit{strong} form. In terms of error analysis, it is more convenient to reformulate it as a (but not always equivalent) \textit{weak} form, see Sect. 1.3.3 and Chap. 4.
\end{itemize}


\subsection{Spectral Methods of Galerkin Type}

The collocation method described in the previous section is implemented in the physical space. In this section, we shall describe Galerkin-type spectral methods in the frequency space, and present hte basic principles of the spectral-Galerkin method, spectral-Petrov-Galerkin method, and spectral-Galerkin method with numerical integration.

\subsubsection{Galerkin Method}

Without loss of generality, we consider (8) with $ g_{ \pm } = 0 $. The non homogeneous boundary conditions can be easily handled by considering $ \nu = u - \tilde{u} $, where $ \tilde{ u } $ is a "simple" function satisfying the non homogeneous boundary conditions (cf. Chap. 4). \\
\indent Define the finite-dimensional approximation space:
\[
	X_{ N } = \{ \phi \in P_{ N }: B_{ \pm } \phi \left( \pm 1 \right) = 0 \} \Rightarrow \mathrm{ dim } \left( X_{ N } \right) = N-1
\]

Let $ \{ \phi_{ k } \}_{ k=0 }^{ N-2 } $ be a set of basis function of $ X_{ N } $. We expand the approximate solution as
\begin{equation}
	u_{ N }(x) = \sum_{ k=0 }^{ N-2 } \widehat{u}_{ k } \phi_{ k } \left( x \right) \in X_{ N }
\end{equation}

Then, the expansion coefficients $ \{ \widehat{ u }_{ k } \}_{ k=0 }^{ N-2 } $ can be determined by by the residual equation (6) with $ \{ \mathbf{ \psi_{ j } = \mathbf{ \phi }_{ j } } \} $ :

\begin{equation}
	\int_{ -1 }^{ 1 } \left( \mathbf{ L } u_{ N }(x) - f(x) \right) \phi_{ j } (x) \omega (x) dx = 0, \hspace{4pt} 0 \leq j \leq N-2
\end{equation}

which is equivalent to finding $ u_{ N } \in X_{ N } $ such that
\begin{equation}
	\left( \mathbf{ L } u_{ N }, \nu_{ N } \right)_{ \omega } = \left( f, \nu_{ N } \right)_{ \omega }, \hspace{4pt} \forall \nu_{ N } \in X_{ N }
\end{equation}

Here $ \left( \cdot, \cdot \right)_{ \omega } $ is the inner system product of $ L^{ 2 }_{ \omega } \left( -1,1 \right) $ (cf. Appendix B).
\indent The linear system of the above scheme is obtained by substituting by substituting (19) into (20). More precisely, setting
\[
	\mathbf{ u } = \left( \widehat{ u }_{ 0 }, \widehat{ u }_{ 1 }, \ldots, \widehat{ u }_{ N-2 } \right)^{ T }
\]

\[
	f_{ j } = \left( f, \phi_{ j } \right)_{ \omega }
\]

\[
	\mathbf{ f } = \left( f_{ 0 }, f_{ 1 }, \ldots, f_{ N-2 } \right)^{ T }
\]
\[
	s_{ jk } = \left( \mathbf{ L } \phi_{ k }, \phi_{ j } \right)_{ \omega }
\]
\[
	S = \left( s_{ jk } \right)_{ j,k=0,\ldots,N-2 }
\]
the system (20) reduces to 
\[
S \mathbf{ u } = \mathbf{ f }
\]

Therefore, it is crucial to choose basis functions $ \{ \phi_{ j } \} $ such that:

\begin{itemize}
	\item the right-hand side $ \left( f, \phi_{ j } \right)_{ \omega } $ can be computed efficiently.
	\item The linear system (22) can be solved efficiently.
\end{itemize}

The key idea is to used \textit{compact combinations} of orthogonal polynomials or orthogonal functions to construct basis functions. To demonstrate the basic principle, we consider the Legendre spectral approximation (i.e. $ \omega \equiv 1 $ in (20)-(22)). Let $ L_{ k }(x) $ be the Legendre polynomial of degree $ k $ , and set

\begin{equation}
	\phi_{ k }(x) = L_{ k }(x) + \alpha_{ k } L_{ k+1 } (x) + \beta_{ k } L_{ k+2 } (x), \hspace{4pt} k \geq 0
\end{equation}

Where the constants $ \alpha_{ k } $ and $ \beta_{ k } $ are uniquely determined by the boundary conditions: $ B_{ \pm } \phi_{ k } \left( \pm 1 \right) = 0 $ (cf. Sect. 4.1). We shall refer to such basis functions as \textit{modal} basis functions. Therefore, we have

\begin{equation}
	X_{ N } = \mathrm{ span } \{ \phi_{ 0 }, \phi_{ 1 }, \ldots, \phi_{ N-2 } \}
\end{equation}
\indent Using the properties of the Legendre polynomials (cf. Sect. 3.3), one verifies easily that, if $ p(x) $ and $ q(x) $  are constants, the coefficient matrix $ S $ is \textit{sparse} so the linear system (22) can be solved efficiently. However, for more general $ p(x) $ and $ q(x) $, the coefficient matrix $ S $ is full an done needs to resort to an iterative method (cf. Sect. 4.4). \\
\indent In the case above, we just considered the Legendre case. In fact, th construction of such a basis is also feasible for the Chebyshev, Laguerre, and Hermite cases (see Chaps. 4-7). The notion of using compact combinations of orthogonal polynomials/functions to develop efficient spectral solvers will be repeatedly emphasized in this book. \\
\indent We now consider the evaluation of $ \left( f, \phi_{ j } \right)_{ \omega } $ . In general, this term can not be computed exactly and is usually approximated by $ \left( I_{ N }f, \phi_{ j } \right)_{ \omega } $, where $ I_{ N } $ is an interpolation operator upon $ P_{ N } $ relative to the Gauss-Lobatto Points. Thus, we can write
\begin{equation}
	\left( I_{ N } f \right) (x) = \sum_{ k=0 }^{ N } \tilde{ f }_{ k } \phi_{ k } (x)
\end{equation}

Where $ \{ \phi_{ k } \} $ is an orthonormal polynomial of $ P_{ N } $ (orthogonal with respect to $ \omega $ , i.e. $ \left( \phi_{ k }, \phi_{ j } \right)_{ \omega } = \delta_{ jk } $ ). Thanks to orthogonality, the \textit{discrete transforms} between the physical values $ \{ f(x_{ j }) \}_{ j=0 }^{ N } $ and the expansion coefficients $ \{ f(x_{ j }) \}_{ j=0 }^{ N } $ can be computed efficiently. In particular, the computational complexity of using the Fourier and Chebyshev discrete transforms can be reduced to $ O(N \mathrm{ log }_{ 2 } N) $ by using the fast Fourier transform (FFT). An approach for implementing discrete transforms relative to general orthogonal polynomials is given in Sec. 3.1.5. \\
\indent It is important to point out that insolving time-dependent nonlinear problems, $ f $ usually contains nonlinear terms involving derivatives of hte numerical solution $ u_{ N } $ at previous time steps (cf. (3)). Hence, numerical differentiations in the frequency space and/or in the physical space are required. Differentiation techniques relative to general orthogonal polynomials are addressed in Sects. 3.1.6 and  3.1.7.\\

\subsubsection{Petrov-Galerkin Method}

As pointed out in Sect. 1.1, the use of different test and trial functions distinguishes the Petrov-Galerkin method from the Galerkin method. Thanks to this flexibility, the Petrov-Galerkin method can be very useful for some non-self-adjoint problems such as odd-order equations. \\
\indent As an illustrative example, we consider the following third-order equation:
\begin{equation}
	\mathbf{ L } u(x) := u'''(x) + u(x) = f(x)\, \hspace{4pt} x \in (-1,1)
\end{equation}
\[
	u( \pm 1) = u'(1) = 0
\]

As with the Galerkin case, we enforce the boundary conditions on the approximate solution. So we set
\[
	X_{ N } = \{ \phi \in P_{ N } \vert \phi( \pm 1) = \phi' (1) = 0 \} \Rightarrow \dim (X_{ N }) = N-2
\]

Assuming that $ \{ \phi_{ k } \}_{ k=0 }^{ N-3 } $ is a basis of $ X_{ N } $ are dtermined by the residual equation (6) (with $ \omega = 1 $ ):

\begin{equation}
	\int_{ -1 }^{ 1 } \left( \mathbf{ L } u_{ N } (x) - f(x) \right) \psi_{ j } (x) dx = 0, \hspace{4pt} 0 \leq j \leq N-3
\end{equation}

Since the leading third-odrer operator is not self-adjoint, it is natural to use a Petrov-Galerkin method with the test function space:

\[
	X_{ N }^{ * } = \{ \psi \in P_{ N } \vert \psi( \pm 1) = \psi'(-1) = 0 \} \Rightarrow \dim ( X^{ * }_{ N } ) = N-2
\]

Assume that $ \{ \psi_{ k } \}_{ k=0 }^{ N-3 } $ is a basis of $ X^{ * }_{ N } $ . Then, (27) is equivalent to the variational formulation:

\begin{center}
	Find $ u_{ N } \in X_{ N } $ such that
	\[
		\left( \mathbf{ L }u_{ N }, \nu_{ N } \right) = \left( f, \nu_{ N } \right) \hspace{4pt} \forall \nu_{ N } \in X^{ * }_{ N }
	\]
\end{center}

Where $ \left( \cdot, \cdot \right) $ is the inner product of the usual $ L^{ 2 } $ -space. \\
\indent The theoretical aspects of the above scheme will be examined in Chap. 6. We now consider its implementation. Setting

\[
	\mathbf{ u } = \left( \widehat{ u }_{ 0 }, \widehat{ u }_{ 1 }, \ldots, \widehat{ u }_{ N-3 } \right)^{ T }
\]

\[
	f_{ j } = \left( f, \psi_{ j } \right)
\]

\[
	\mathbf{ f } \left( f_{ 0 }, f_{ 1 }, \ldots, f_{ N-3 } \right)^{ T };
\]

\[
	s_{ jk } = \left( \phi_{ k }', \psi_{ j }'' \right)
\]

\[
	S = \left( s_{ jk } \right)_{ j,k=0,\ldots , N-3 }
\]

\[
	m_{ jk } = \left( \phi_{ k }, \psi_{ j } \right)
\]

\[
	M = \left( m_{ jk } \right)_{ j,k=0,\ldots,N-3 }
\]

The linear system becomes

\begin{equation}
	(S+M) \mathbf{ u } = \mathbf{ f }
\end{equation}

As described in the previous section, we wish to construct basis functions for $ X_{ N } $ and $ X_{ N }^{ * } $ so that the linear system (29) can be inverted efficiently. Once again, this goal can be achieved by using compact combinations of orthogonal polynomials. It can be checked that for $ 0 \leq k \leq N-3 $ ,  

\[
	\phi_{ k } =L_{ k } - \frac{2k+3}{2k+5} L_{ k+1 } - L_{ k+2 } + \frac{2k+3}{2k+5} L_{ k+3 } \in X_{ N }
\]
